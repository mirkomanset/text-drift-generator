{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "while not os.getcwd().endswith(\"text-drift-generator\"):\n",
    "    os.chdir(\"..\")\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from scripts.enums import WordErrorType\n",
    "from scripts.preprocess import preprocess_text\n",
    "from scripts.drift_generator import simulate_drift\n",
    "from scripts.constants import EXAMPLE_STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test simulate error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating typographical errors:\n",
      "Stage 0.1: Artificial intellignce is revolutionizing the way we live and work. It is transforming industries yb automating tasks, impzproving efficiency, and enhancing decision-making. As AI technology evolves, it opens up new possibilities for businesses and individuals avlike. However, challenges remain in ensuring ethical use, data privacy, and fairness. As we move forward, it's crucial to balance innovation with responsibility to maximize the benefits while minimizing ridks.\n",
      "Stage 0.5: Ahrtifuial intelligence is gevuolutonizing the ay we liv eagnd work. It ist ransforming industqries by automating taaks, improving efficeinc,y ad enhancijg decjsion-makngi. As Au technology evopve,si t opens up new posziblites for vbusinesses and iyndividualss azlgike. However, challengew remain iln ensuring ethical uet, data privacy, gand fairnecss. A we move forward, it's crucial to balance innovaion wigh respjonsibilityv to mazomize the benehfits wile minimizing risks.\n",
      "Stage 0.9: Ahrtifuial intelljgenxe iz urevoutionizgingthe way w elgive and work. tIis transfyotming nidusriepzsb y akyomating takss,i mpoving effjciencg,and enhanicng decsiion-makigng. w sIA ehcnology evolcsc, tit opes utp new pkyfssiibltiizsg for busiezsses adn indivdulals aflioe. Howevrt, challemges rgemain in nesuingethical ues, data prvacy, and fairness. Ahs w kve forwardj, it's cruvciwl to galance innovahtion wthresponsdibility to maximjazeu te benefits while minimizing risks.\n",
      "\n",
      "Simulating phonetic errors:\n",
      "Stage 0.1: Artificial intelligence is revolutionizing the way we live and work. It 1 transforming industries by automating tasks, improving efficiency, and enhancing decision-making. As AI technology evolves, it opens up new possibilities for businesses and individuals alike. However, challenges remain in ensuring ethical use, data privacy, and fairness. a we move forward, it's crucial to balance innovation with responsibility too maximize the benefits while minimizing risks.\n",
      "Stage 0.5: eye intelligence is u the e we live and work. It is transforming industries by oa tasks, improving efficiency, and e decision-making. As AI technology 0 it opens up new possibilities for yoo and individuals alike. However, challenges remain in ensuring ethical use, data privacy, and fairness. As we ea forward, it's crucial to balance innovation with responsibility to eh the benefits while minimizing risks.\n",
      "Stage 0.9: eye i is o da way we live and work. ai is oh industries by automating tasks, improving 3 and enhancing ea As AI oa evolves, it opens up e oh for you and individuals alike. o e eh in ensuring ea use, a privacy, and a As we 3 4r it's @ to ah oa with eh to maximize the benefits e minimizing risks.\n",
      "\n",
      "Simulating mix errors:\n",
      "Stage 0.1: Artificial intelligence is revolutionizing the way we live and work. It is transforming industries by automating tasks, imprpoving efficiency, and enhancing decision-making. As AI technology evolves, it opens up new possibilities for businesses and individuals alike. However, challenges rmain in ensuring ethical use, data privacy, and fairness. As we move forward it's crucial to balgance innovation with responsibility to eye the benefits while minimizing risks.\n",
      "Stage 0.5: Artificial intellignce is revolutionizing the way we live and work. It 1 transforming industries yb automating tasks, impzproving efficiency, and enhancing decision-making. As AI technology evolves, it opens up new possibilities for businesses and individuals avlike. However, challenges remain in ensuring ethical use, data privacy, and fairness. a we move forward, it's crucial to balance innovation with responsibility too maximize the benefits while minimizing ridks.\n",
      "Stage 0.9: eye intelligence is u the e we live and work. It is transforming industries by autoqmating tasks, improving efficiency, and enhacnig decision-making. As AI technology evolves, it opens up new possibilities for businesses and individuals valike. However, challenges remain in ensuring ethical use, data privacy, and faifrness. As we move forward, it's @ to balance innovation with responsibility to maximize the benefits while minimizinhj risks.\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "text = EXAMPLE_STRING\n",
    "error_types = [\n",
    "    WordErrorType.TYPOGRAPHICAL,\n",
    "    WordErrorType.PHONETIC,\n",
    "    WordErrorType.MIX,\n",
    "]\n",
    "\n",
    "levels = [0.1, 0.5, 0.9]\n",
    "\n",
    "for error_type in error_types:\n",
    "    print(f\"\\nSimulating {error_type} errors:\")\n",
    "    for level in levels:\n",
    "        drifted_text = simulate_drift(text, error_type, level)\n",
    "        print(f\"Stage {level}: {drifted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['comp.graphics', 'rec.autos', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.med', 'talk.politics.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    \"comp.graphics\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.med\",\n",
    "    \"talk.politics.misc\",\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=False,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.1, 0.5, 0.9]\n",
    "full_dfs = []\n",
    "error_type = WordErrorType.TYPOGRAPHICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply drift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVEL 0.1\n",
      "Done\n",
      "LEVEL 0.5\n",
      "Done\n",
      "LEVEL 0.9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for level in levels:\n",
    "    print(f\"LEVEL {level}\")\n",
    "\n",
    "    # Preprocess text data and remove empty samples\n",
    "    original_texts = []\n",
    "    original_preprocessed_texts = []\n",
    "    drifted_texts = []\n",
    "    drifted_preprocessed_texts = []\n",
    "    targets = []\n",
    "\n",
    "    for text, target in zip(df.data, df.target):\n",
    "        # Aplly drift to data\n",
    "        drifted_text = simulate_drift(text, error_type, level=level)\n",
    "        if drifted_text:  # Check if text is not empty after preprocessing\n",
    "            original_preprocessed_text = preprocess_text(text)\n",
    "            drifted_preprocessed_text = preprocess_text(drifted_text)\n",
    "\n",
    "            if (\n",
    "                original_preprocessed_text and drifted_preprocessed_text\n",
    "            ):  # Check if text is not empty after preprocessing\n",
    "                original_texts.append(text)\n",
    "                original_preprocessed_texts.append(original_preprocessed_text)\n",
    "                drifted_texts.append(drifted_text)\n",
    "                drifted_preprocessed_texts.append(drifted_preprocessed_text)\n",
    "                targets.append(target)\n",
    "\n",
    "    # Create a new DataFrame with preprocessed data\n",
    "    full_df = pd.DataFrame(\n",
    "        {\n",
    "            \"original_data\": original_texts,\n",
    "            \"original_preprocessed_data\": original_preprocessed_texts,\n",
    "            \"drifted_data\": drifted_texts,\n",
    "            \"drifted_preprocessed_data\": drifted_preprocessed_texts,\n",
    "            \"target\": targets,\n",
    "        }\n",
    "    )\n",
    "    full_dfs.append(full_df)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:\n",
      "\n",
      "morgan and guzman will have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and not pitch harkey as much as hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher) \n",
      "\n",
      "ORIGINAL PREPROCESSED TEXT:\n",
      "morgan guzman era's 1 run higher last year cub idiot pitch harkey much hibbard castillo good think stud pitcher \n",
      "\n",
      "LEVEL 0.1\n",
      "DRIFTED TEXT:\n",
      "\n",
      "morgan and guzman ill have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and no tpitch harkey as much pzas hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher) \n",
      "\n",
      "DRIFTED PREPROCESSED TEXT\n",
      "morgan guzman ill era's 1 run higher last year cub idiot tpitch harkey much pzas hibbard castillo good think stud pitcher\n",
      "-----------------------------------------------\n",
      "LEVEL 0.5\n",
      "DRIFTED TEXT:\n",
      "\n",
      "hmorgq and guzman will havf eura's1 run higherthan lasty egar, and\n",
      " the cbus will be idiots aqnd not pitch harkey aa much as hibbard.\n",
      " acstlilowon't be yood (j think h'se a stus pitcher) \n",
      "\n",
      "DRIFTED PREPROCESSED TEXT\n",
      "hmorgq guzman havf eura's1 run higherthan lasty egar cbus idiot aqnd pitch harkey aa much hibbard acstlilowon't yood j think h'se stus pitcher\n",
      "-----------------------------------------------\n",
      "LEVEL 0.9\n",
      "DRIFTED TEXT:\n",
      "\n",
      "hmorgq and guzmaj wikl hqve era's1 run hgighr than lats gyear, and\n",
      " thec bs will bye diiot sandnotpz iotcb harkey as mcuh sa hbbard.\n",
      " csstillk on't be good (i tihnk he's ga stkd iptcher) \n",
      "\n",
      "DRIFTED PREPROCESSED TEXT\n",
      "hmorgq guzmaj wikl hqve era's1 run hgighr lat gyear thec b bye diiot sandnotpz iotcb harkey mcuh sa hbbard csstillk on't good tihnk ga stkd iptcher\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"ORIGINAL TEXT:\\n{full_dfs[0].iat[0,0]} \\n\")\n",
    "print(f\"ORIGINAL PREPROCESSED TEXT:\\n{full_dfs[0].iat[0,1]} \\n\")\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    full_df = full_dfs[i]\n",
    "    print(f\"LEVEL {levels[i]}\")\n",
    "    print(f\"DRIFTED TEXT:\\n{full_df.iat[0,2]} \\n\")\n",
    "    print(f\"DRIFTED PREPROCESSED TEXT\\n{full_df.iat[0,3]}\")\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start vectorizing original 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 173/173 [01:56<00:00,  1.48it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/drifted_/data/original_0.1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m X = model.encode(original_data, batch_size=batch_size, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Ensure CPU usage\u001b[39;00m\n\u001b[32m     15\u001b[39m X_list.append(X)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/data/drifted_/data/original_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlevels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstart vectorizing drifted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Encode drifted data in batches\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manse\\OneDrive\\Desktop\\text-drift-generator\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1563\u001b[39m, in \u001b[36msavetxt\u001b[39m\u001b[34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[39m\n\u001b[32m   1560\u001b[39m     fname = os.fspath(fname)\n\u001b[32m   1561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_string_like(fname):\n\u001b[32m   1562\u001b[39m     \u001b[38;5;66;03m# datasource doesn't support creating a new file ...\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1563\u001b[39m     \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.close()\n\u001b[32m   1564\u001b[39m     fh = np.lib._datasource.open(fname, \u001b[33m'\u001b[39m\u001b[33mwt\u001b[39m\u001b[33m'\u001b[39m, encoding=encoding)\n\u001b[32m   1565\u001b[39m     own_fh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data/drifted_/data/original_0.1.csv'"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare your data in batches for more efficient encoding\n",
    "batch_size = 32  # You can adjust this based on your system's capacity\n",
    "\n",
    "X_drifted_list = []\n",
    "X_list = []\n",
    "\n",
    "for i in range(len(full_dfs)):\n",
    "    print(f\"start vectorizing original {levels[i]}\")\n",
    "    # Encode in batches\n",
    "    original_data = full_dfs[i].original_preprocessed_data\n",
    "    X = model.encode(original_data, batch_size=batch_size, show_progress_bar=True, device='cpu')  # Ensure CPU usage\n",
    "    X_list.append(X)\n",
    "    np.savetxt(f'/data/drifted_data/original_{levels[i]}.csv', X, delimiter=',')\n",
    "\n",
    "    print(f\"start vectorizing drifted {levels[i]}\")\n",
    "    # Encode drifted data in batches\n",
    "    drifted_data = full_dfs[i].drifted_preprocessed_data\n",
    "    X_drifted = model.encode(drifted_data, batch_size=batch_size, show_progress_bar=True, device='cpu')  # Ensure CPU usage\n",
    "    X_drifted_list.append(X_drifted)\n",
    "    np.savetxt(f'/data/drifted_data/drifted_{levels[i]}.csv', X_drifted, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 1.0000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5533 is out of bounds for axis 0 with size 5533",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m labels = []  \u001b[38;5;66;03m# Labels for the boxplots\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, X_drifted_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_drifted_list):  \u001b[38;5;66;03m# Loop through each drifted dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     cosine_similarities = \u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_drifted_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m     cosine_similarities_list.append(cosine_similarities)\n\u001b[32m     10\u001b[39m     labels.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDrift level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Label each drifted dataset\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      3\u001b[39m labels = []  \u001b[38;5;66;03m# Labels for the boxplots\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, X_drifted_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_drifted_list):  \u001b[38;5;66;03m# Loop through each drifted dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m     cosine_similarities = [\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         cosine_similarity([X[i]], [\u001b[43mX_drifted_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m])[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))\n\u001b[32m      8\u001b[39m     ]\n\u001b[32m      9\u001b[39m     cosine_similarities_list.append(cosine_similarities)\n\u001b[32m     10\u001b[39m     labels.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDrift level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Label each drifted dataset\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: index 5533 is out of bounds for axis 0 with size 5533"
     ]
    }
   ],
   "source": [
    "# Assuming X is your reference dataset and X_drifted is a list of drifted datasets\n",
    "cosine_similarities_list = []  # List to store cosine similarities for each drifted dataset\n",
    "labels = []  # Labels for the boxplots\n",
    "\n",
    "for idx in len(range(full_dfs)):  # Loop through each drifted dataset\n",
    "    cosine_similarities = [\n",
    "        cosine_similarity([X_list[idx][i]], [X_drifted[idx][i]])[0][0] for i in range(len(X_list[idx]))\n",
    "    ]\n",
    "    cosine_similarities_list.append(cosine_similarities)\n",
    "    labels.append(f\"Drift level {idx}\")  # Label each drifted dataset\n",
    "    average_similarity = np.mean(cosine_similarities)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n",
    "\n",
    "# Plot the boxplots for each drifted dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=cosine_similarities_list)\n",
    "plt.xticks(ticks=range(len(labels)), labels=labels)\n",
    "plt.title(\"Cosine Similarity Distribution for Different Drifted Datasets\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.xlabel(\"Drifted Datasets\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
